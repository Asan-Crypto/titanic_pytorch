{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Проект: датасет titanic. Поиск выживших"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"train.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Все пропущенные значения дополним нулем\n",
    "df.Age = df.Age.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Убираем неинформотивные фичи\n",
    "X = df.drop({'PassengerId', 'Survived', 'Name', 'Ticket', 'Cabin', 'Sex', 'Embarked'}, axis = 1)\n",
    "y = df.Survived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Переводим все на torch\n",
    "X_train = torch.from_numpy(X_train.to_numpy())\n",
    "y_train = torch.from_numpy(y_train.to_numpy())\n",
    "X_test = torch.from_numpy(X_test.to_numpy())\n",
    "y_test = torch.from_numpy(y_test.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем класс Dataset\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, datamatrix, labels):\n",
    "        self.datamatrix = datamatrix\n",
    "        self.datamatrix = datamatrix/datamatrix.max(0).values\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __getitem__(self, idx): # обязательный\n",
    "        return self.datamatrix[idx].float(), self.labels[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.datamatrix.shape[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.0000, 47.0000,  0.0000,  0.0000,  9.0000],\n",
       "        [ 3.0000,  0.0000,  0.0000,  0.0000,  7.7500],\n",
       "        [ 2.0000, 34.0000,  1.0000,  0.0000, 26.0000],\n",
       "        ...,\n",
       "        [ 3.0000, 19.0000,  0.0000,  0.0000,  6.7500],\n",
       "        [ 3.0000,  0.0000,  1.0000,  0.0000, 15.5000],\n",
       "        [ 3.0000, 21.0000,  0.0000,  0.0000,  7.8542]], dtype=torch.float64)"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "md_train = MyDataset(X_train, y_train)\n",
    "md_test = MyDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Определяем размер батча\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Заранее предобрабатываем данные\n",
    "train_dataloader = DataLoader(md_train, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(md_test, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a, b in test_dataloader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 5]), torch.Size([32]))"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape, b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем класс Model\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, n_hid):\n",
    "        super().__init__()\n",
    "        self.fc1 = torch.nn.Linear(5, n_hid)\n",
    "        self.act1 = torch.nn.ReLU()\n",
    "        self.fc2 = torch.nn.Linear(n_hid, n_hid)\n",
    "        self.act2 = torch.nn.ReLU()\n",
    "        self.fc3 = torch.nn.Linear(n_hid, 1)    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.act2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Попробуем 100 нейронов в скрытых слоях\n",
    "# в качестве оптимайзера используем Adam и скорость обучение возбмем равной 0.001\n",
    "# loss - BCEWithLogitsLoss\n",
    "net = Model(100)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr= 0.001)\n",
    "loss_func = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net, optimizer, loss_func, train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.7472370862960815\n",
      "Loss:  0.7291766405105591\n",
      "Loss:  0.7121816277503967\n",
      "Loss:  0.6950320601463318\n",
      "Loss:  0.6917250156402588\n",
      "Loss:  0.6876280307769775\n",
      "Loss:  0.6778491139411926\n",
      "Loss:  0.673558235168457\n",
      "Loss:  0.6852138042449951\n",
      "Loss:  0.6977835297584534\n",
      "Loss:  0.692570686340332\n",
      "Loss:  0.6307507753372192\n",
      "Loss:  0.651593804359436\n",
      "Loss:  0.6256078481674194\n",
      "Loss:  0.6067615747451782\n",
      "Loss:  0.641115665435791\n",
      "Loss:  0.6884005665779114\n",
      "Loss:  0.7283017635345459\n",
      "Loss:  0.6930909752845764\n",
      "Loss:  0.6373348832130432\n",
      "Loss:  0.670659601688385\n",
      "Loss:  0.6707863807678223\n",
      "Loss:  0.5752269625663757\n",
      "Loss:  0.5981974005699158\n",
      "Loss:  0.7004155516624451\n",
      "Loss:  0.6845209002494812\n",
      "Loss:  0.6332600712776184\n",
      "Loss:  0.6217753887176514\n",
      "Loss:  0.5994395613670349\n",
      "Loss:  0.6523075103759766\n",
      "Loss:  0.6305676102638245\n",
      "Loss:  0.6849724054336548\n",
      "Loss:  0.5542402863502502\n",
      "Loss:  0.5857157111167908\n",
      "Loss:  0.5772216320037842\n",
      "Loss:  0.612677812576294\n",
      "Loss:  0.6544365882873535\n",
      "Loss:  0.5467052459716797\n",
      "Loss:  0.8208664655685425\n",
      "Loss:  0.7295014262199402\n",
      "Loss:  0.6921996474266052\n",
      "Loss:  0.7405062317848206\n",
      "Loss:  0.6334154605865479\n",
      "Loss:  0.645077109336853\n",
      "Loss:  0.6723204851150513\n",
      "Loss:  0.6249018311500549\n",
      "Loss:  0.5732429623603821\n",
      "Loss:  0.670015811920166\n",
      "Loss:  0.5943495631217957\n",
      "Loss:  0.6113784313201904\n",
      "Loss:  0.5360056161880493\n",
      "Loss:  0.7084787487983704\n",
      "Loss:  0.6333902478218079\n",
      "Loss:  0.7434827089309692\n",
      "Loss:  0.6271580457687378\n",
      "Loss:  0.6339858770370483\n",
      "Loss:  0.6291865706443787\n",
      "Loss:  0.6906836032867432\n",
      "Loss:  0.672236442565918\n",
      "Loss:  0.5935588479042053\n",
      "Loss:  0.6943603157997131\n",
      "Loss:  0.6855064630508423\n",
      "Loss:  0.5305781364440918\n",
      "Loss:  0.646575391292572\n",
      "Loss:  0.6123511791229248\n",
      "Loss:  0.6135345697402954\n",
      "Loss:  0.7113608121871948\n",
      "Loss:  0.5706775188446045\n",
      "Loss:  0.5932317972183228\n",
      "Loss:  0.6626275777816772\n",
      "Loss:  0.6233704090118408\n",
      "Loss:  0.6162977814674377\n",
      "Loss:  0.6793230175971985\n",
      "Loss:  0.6247726082801819\n",
      "Loss:  0.5627246499061584\n",
      "Loss:  0.5946282744407654\n",
      "Loss:  0.5817956328392029\n",
      "Loss:  0.6334673166275024\n",
      "Loss:  0.5963457822799683\n",
      "Loss:  0.637222409248352\n",
      "Loss:  0.6451094150543213\n",
      "Loss:  0.6487489938735962\n",
      "Loss:  0.5971307754516602\n",
      "Loss:  0.6072033643722534\n",
      "Loss:  0.5751858353614807\n",
      "Loss:  0.7068952322006226\n",
      "Loss:  0.7085363864898682\n",
      "Loss:  0.5597548484802246\n",
      "Loss:  0.6937496662139893\n",
      "Loss:  0.5924131274223328\n",
      "Loss:  0.6098234057426453\n",
      "Loss:  0.40207043290138245\n",
      "Loss:  0.561069667339325\n",
      "Loss:  0.5408790707588196\n",
      "Loss:  0.6210657358169556\n",
      "Loss:  0.5612626075744629\n",
      "Loss:  0.53384929895401\n",
      "Loss:  0.5010836124420166\n",
      "Loss:  0.6720939874649048\n",
      "Loss:  0.7211865186691284\n",
      "Loss:  0.6075484156608582\n",
      "Loss:  0.5832026600837708\n",
      "Loss:  0.6123908758163452\n",
      "Loss:  0.7542969584465027\n",
      "Loss:  0.5816522240638733\n",
      "Loss:  0.7071404457092285\n",
      "Loss:  0.4830954670906067\n",
      "Loss:  0.6302062273025513\n",
      "Loss:  0.6172353029251099\n",
      "Loss:  0.6185027360916138\n",
      "Loss:  0.7025834918022156\n",
      "Loss:  0.628490686416626\n",
      "Loss:  0.6066178679466248\n",
      "Loss:  0.6451915502548218\n",
      "Loss:  0.576794445514679\n",
      "Loss:  0.5894156694412231\n",
      "Loss:  0.5780465602874756\n",
      "Loss:  0.6044883728027344\n",
      "Loss:  0.5429725050926208\n",
      "Loss:  0.5913717150688171\n",
      "Loss:  0.6996399760246277\n",
      "Loss:  0.5714268684387207\n",
      "Loss:  0.6237809658050537\n",
      "Loss:  0.642025887966156\n",
      "Loss:  0.6109853386878967\n",
      "Loss:  0.633188784122467\n",
      "Loss:  0.7044380903244019\n",
      "Loss:  0.61977618932724\n",
      "Loss:  0.5861619710922241\n",
      "Loss:  0.5262948274612427\n",
      "Loss:  0.6610536575317383\n",
      "Loss:  0.5865298509597778\n",
      "Loss:  0.5445375442504883\n",
      "Loss:  0.6152942776679993\n",
      "Loss:  0.5518508553504944\n",
      "Loss:  0.5349407196044922\n",
      "Loss:  0.7345058917999268\n",
      "Loss:  0.6095120906829834\n",
      "Loss:  0.7093998193740845\n",
      "Loss:  0.5834105014801025\n",
      "Loss:  0.5560760498046875\n",
      "Loss:  0.6428695917129517\n",
      "Loss:  0.5824723839759827\n",
      "Loss:  0.5803134441375732\n",
      "Loss:  0.6087790727615356\n",
      "Loss:  0.5521284341812134\n",
      "Loss:  0.5976464748382568\n",
      "Loss:  0.6473605632781982\n",
      "Loss:  0.5878357291221619\n",
      "Loss:  0.6199211478233337\n",
      "Loss:  0.6602917313575745\n",
      "Loss:  0.6260852813720703\n",
      "Loss:  0.5110419392585754\n",
      "Loss:  0.5678706169128418\n",
      "Loss:  0.5756846070289612\n",
      "Loss:  0.6280584335327148\n",
      "Loss:  0.5007514953613281\n",
      "Loss:  0.6494789719581604\n",
      "Loss:  0.5846570134162903\n",
      "Loss:  0.6167495846748352\n",
      "Loss:  0.4686385989189148\n",
      "Loss:  0.5627835392951965\n",
      "Loss:  0.5830678939819336\n",
      "Loss:  0.5790006518363953\n",
      "Loss:  0.537310004234314\n",
      "Loss:  0.5767467021942139\n",
      "Loss:  0.6952307224273682\n",
      "Loss:  0.6429586410522461\n",
      "Loss:  0.5582333207130432\n",
      "Loss:  0.5649051070213318\n",
      "Loss:  0.6066555380821228\n",
      "Loss:  0.523882269859314\n",
      "Loss:  0.6221449971199036\n",
      "Loss:  0.5076166391372681\n",
      "Loss:  0.5867766737937927\n",
      "Loss:  0.5147879123687744\n",
      "Loss:  0.5773054361343384\n",
      "Loss:  0.6473222374916077\n",
      "Loss:  0.642584502696991\n",
      "Loss:  0.44774091243743896\n",
      "Loss:  0.5374240875244141\n",
      "Loss:  0.7420231699943542\n",
      "Loss:  0.7232578992843628\n",
      "Loss:  0.8700180053710938\n",
      "Loss:  0.509113073348999\n",
      "Loss:  0.7049059271812439\n",
      "Loss:  0.554102897644043\n",
      "Loss:  0.5646286606788635\n",
      "Loss:  0.5300507545471191\n",
      "Loss:  0.5746225714683533\n",
      "Loss:  0.6895949244499207\n",
      "Loss:  0.5022680163383484\n",
      "Loss:  0.6614852547645569\n",
      "Loss:  0.7018793821334839\n",
      "Loss:  0.651200532913208\n",
      "Loss:  0.6088240742683411\n",
      "Loss:  0.5262024998664856\n",
      "Loss:  0.5727791786193848\n",
      "Loss:  0.5338543653488159\n",
      "Loss:  0.5782811045646667\n",
      "Loss:  0.6526625156402588\n",
      "Loss:  0.5696291327476501\n",
      "Loss:  0.5831080675125122\n",
      "Loss:  0.5751080513000488\n",
      "Loss:  0.4453272223472595\n",
      "Loss:  0.684544563293457\n",
      "Loss:  0.4768027067184448\n",
      "Loss:  0.6313683390617371\n",
      "Loss:  0.4381002187728882\n",
      "Loss:  0.5921822190284729\n",
      "Loss:  0.7113841772079468\n",
      "Loss:  0.5665138959884644\n",
      "Loss:  0.4586596190929413\n",
      "Loss:  0.6470268964767456\n",
      "Loss:  0.6823651790618896\n",
      "Loss:  0.5046926736831665\n",
      "Loss:  0.6051620244979858\n",
      "Loss:  0.6768768429756165\n",
      "Loss:  0.5570405125617981\n",
      "Loss:  0.5841755867004395\n",
      "Loss:  0.6559917330741882\n",
      "Loss:  0.6339011192321777\n",
      "Loss:  0.604033887386322\n",
      "Loss:  0.5864745378494263\n",
      "Loss:  0.5133829712867737\n",
      "Loss:  0.5418896079063416\n",
      "Loss:  0.6081555485725403\n",
      "Loss:  0.6401568651199341\n",
      "Loss:  0.5241693258285522\n",
      "Loss:  0.5148990154266357\n",
      "Loss:  0.6162238121032715\n",
      "Loss:  0.5901386737823486\n",
      "Loss:  0.537924587726593\n",
      "Loss:  0.5002729892730713\n",
      "Loss:  0.5123409628868103\n",
      "Loss:  0.6707724332809448\n",
      "Loss:  0.5020898580551147\n",
      "Loss:  0.6802158951759338\n",
      "Loss:  0.5490013360977173\n",
      "Loss:  0.5588653683662415\n",
      "Loss:  0.5062875747680664\n",
      "Loss:  0.594873309135437\n",
      "Loss:  0.6678472757339478\n",
      "Loss:  0.5937194228172302\n",
      "Loss:  0.5139963030815125\n",
      "Loss:  0.7024537324905396\n",
      "Loss:  0.5456884503364563\n",
      "Loss:  0.5017056465148926\n",
      "Loss:  0.6429063081741333\n",
      "Loss:  0.5592730045318604\n",
      "Loss:  0.5766354203224182\n",
      "Loss:  0.7047056555747986\n",
      "Loss:  0.3905089795589447\n",
      "Loss:  0.6031230688095093\n",
      "Loss:  0.474742591381073\n",
      "Loss:  0.6272733211517334\n",
      "Loss:  0.6653319597244263\n",
      "Loss:  0.5920422673225403\n",
      "Loss:  0.4854416251182556\n",
      "Loss:  0.6193506121635437\n",
      "Loss:  0.5973143577575684\n",
      "Loss:  0.6239511370658875\n",
      "Loss:  0.5358309149742126\n",
      "Loss:  0.596158504486084\n",
      "Loss:  0.551304280757904\n",
      "Loss:  0.6342663764953613\n",
      "Loss:  0.43816903233528137\n",
      "Loss:  0.7118231058120728\n",
      "Loss:  0.6415570974349976\n",
      "Loss:  0.5132811069488525\n",
      "Loss:  0.6227334141731262\n",
      "Loss:  0.509692370891571\n",
      "Loss:  0.6373112201690674\n",
      "Loss:  0.5396502017974854\n",
      "Loss:  0.5221886038780212\n",
      "Loss:  0.5283496975898743\n",
      "Loss:  0.7593414783477783\n",
      "Loss:  0.599812388420105\n",
      "Loss:  0.5769315361976624\n",
      "Loss:  0.571840226650238\n",
      "Loss:  0.63275146484375\n",
      "Loss:  0.5660130381584167\n",
      "Loss:  0.6293012499809265\n",
      "Loss:  0.44467198848724365\n",
      "Loss:  0.6524111032485962\n",
      "Loss:  0.5174154043197632\n",
      "Loss:  0.5946510434150696\n",
      "Loss:  0.6288750171661377\n",
      "Loss:  0.5359643697738647\n",
      "Loss:  0.49950143694877625\n",
      "Loss:  0.537713348865509\n",
      "Loss:  0.4588308334350586\n",
      "Loss:  0.5693032145500183\n",
      "Loss:  0.5819510221481323\n",
      "Loss:  0.6561586856842041\n",
      "Loss:  0.6912879943847656\n",
      "Loss:  0.5777086615562439\n",
      "Loss:  0.6014130711555481\n",
      "Loss:  0.49647730588912964\n",
      "Loss:  0.5779226422309875\n",
      "Loss:  0.58565753698349\n",
      "Loss:  0.6010887026786804\n",
      "Loss:  0.7759556770324707\n",
      "Loss:  0.5341458916664124\n",
      "Loss:  0.6018949151039124\n",
      "Loss:  0.5895999073982239\n",
      "Loss:  0.562166154384613\n",
      "Loss:  0.6297668218612671\n",
      "Loss:  0.46784549951553345\n",
      "Loss:  0.5341775417327881\n",
      "Loss:  0.5402230620384216\n",
      "Loss:  0.6146397590637207\n",
      "Loss:  0.47064724564552307\n",
      "Loss:  0.4785936176776886\n",
      "Loss:  0.542112410068512\n",
      "Loss:  0.6913881301879883\n",
      "Loss:  0.5495153665542603\n",
      "Loss:  0.5025651454925537\n",
      "Loss:  0.575870931148529\n",
      "Loss:  0.7051981091499329\n",
      "Loss:  0.5708163976669312\n",
      "Loss:  0.521647572517395\n",
      "Loss:  0.49809831380844116\n",
      "Loss:  0.583077609539032\n",
      "Loss:  0.45367753505706787\n",
      "Loss:  0.626541793346405\n",
      "Loss:  0.5968450307846069\n",
      "Loss:  0.5791658163070679\n",
      "Loss:  0.7330141067504883\n",
      "Loss:  0.5515338182449341\n",
      "Loss:  0.6998178958892822\n",
      "Loss:  0.5924558043479919\n",
      "Loss:  0.7811274528503418\n",
      "Loss:  0.5254939198493958\n",
      "Loss:  0.5947137475013733\n",
      "Loss:  0.5241115093231201\n",
      "Loss:  0.5995981693267822\n",
      "Loss:  0.4838535487651825\n",
      "Loss:  0.4757838249206543\n",
      "Loss:  0.5863111019134521\n",
      "Loss:  0.5719220638275146\n",
      "Loss:  0.4918307065963745\n",
      "Loss:  0.5099144577980042\n",
      "Loss:  0.5426684617996216\n",
      "Loss:  0.6958564519882202\n",
      "Loss:  0.5574472546577454\n",
      "Loss:  0.5541815757751465\n",
      "Loss:  0.493330180644989\n",
      "Loss:  0.6027774810791016\n",
      "Loss:  0.6082882285118103\n",
      "Loss:  0.5122948884963989\n",
      "Loss:  0.45365554094314575\n",
      "Loss:  0.5841309428215027\n",
      "Loss:  0.6095981001853943\n",
      "Loss:  0.7559969425201416\n",
      "Loss:  0.503380537033081\n",
      "Loss:  0.45950308442115784\n",
      "Loss:  0.5433579087257385\n",
      "Loss:  0.5151682496070862\n",
      "Loss:  0.6516731381416321\n",
      "Loss:  0.6742153167724609\n",
      "Loss:  0.6534037590026855\n",
      "Loss:  0.6437963843345642\n",
      "Loss:  0.5971906185150146\n",
      "Loss:  0.4968417286872864\n",
      "Loss:  0.5676218271255493\n",
      "Loss:  0.5441283583641052\n",
      "Loss:  0.4973510801792145\n",
      "Loss:  0.619996190071106\n",
      "Loss:  0.5497789978981018\n",
      "Loss:  0.531461775302887\n",
      "Loss:  0.5143992304801941\n",
      "Loss:  0.7000530958175659\n",
      "Loss:  0.5978344082832336\n",
      "Loss:  0.6891945600509644\n",
      "Loss:  0.4548068344593048\n",
      "Loss:  0.5302110910415649\n",
      "Loss:  0.6832188963890076\n",
      "Loss:  0.5910641551017761\n",
      "Loss:  0.49399206042289734\n",
      "Loss:  0.48143449425697327\n",
      "Loss:  0.5052616000175476\n",
      "Loss:  0.5473594665527344\n",
      "Loss:  0.6236918568611145\n",
      "Loss:  0.691308856010437\n",
      "Loss:  0.6732489466667175\n",
      "Loss:  0.5892908573150635\n",
      "Loss:  0.5790502429008484\n",
      "Loss:  0.4241679310798645\n",
      "Loss:  0.49844080209732056\n",
      "Loss:  0.573188066482544\n",
      "Loss:  0.5052204728126526\n",
      "Loss:  0.5500337481498718\n",
      "Loss:  0.5477402806282043\n",
      "Loss:  0.49669796228408813\n",
      "Loss:  0.6428588032722473\n",
      "Loss:  0.4947691857814789\n",
      "Loss:  0.46023210883140564\n",
      "Loss:  0.6163291335105896\n",
      "Loss:  0.43639135360717773\n",
      "Loss:  0.6405491828918457\n",
      "Loss:  0.774518609046936\n",
      "Loss:  0.6093758940696716\n",
      "Loss:  0.6976374387741089\n",
      "Loss:  0.5045111179351807\n",
      "Loss:  0.4628288149833679\n",
      "Loss:  0.5504802465438843\n",
      "Loss:  0.5364584922790527\n",
      "Loss:  0.6069447994232178\n",
      "Loss:  0.6327111721038818\n",
      "Loss:  0.4833512008190155\n",
      "Loss:  0.5773544311523438\n",
      "Loss:  0.7056621313095093\n",
      "Loss:  0.4960154592990875\n",
      "Loss:  0.47989073395729065\n",
      "Loss:  0.6549019813537598\n",
      "Loss:  0.7510281801223755\n",
      "Loss:  0.5767220258712769\n",
      "Loss:  0.6154855489730835\n",
      "Loss:  0.5328409671783447\n",
      "Loss:  0.5644568204879761\n",
      "Loss:  0.5831357836723328\n",
      "Loss:  0.5706049203872681\n",
      "Loss:  0.5199393630027771\n",
      "Loss:  0.48050856590270996\n",
      "Loss:  0.5792608261108398\n",
      "Loss:  0.6379854083061218\n",
      "Loss:  0.6864587068557739\n",
      "Loss:  0.5375253558158875\n",
      "Loss:  0.5230036973953247\n",
      "Loss:  0.5137364864349365\n",
      "Loss:  0.6533211469650269\n",
      "Loss:  0.4980466961860657\n",
      "Loss:  0.5780825614929199\n",
      "Loss:  0.45472684502601624\n",
      "Loss:  0.5944679975509644\n",
      "Loss:  0.5186445713043213\n",
      "Loss:  0.5077562928199768\n",
      "Loss:  0.5520186424255371\n",
      "Loss:  0.5139261484146118\n",
      "Loss:  0.6351948380470276\n",
      "Loss:  0.5260831117630005\n",
      "Loss:  0.528916597366333\n",
      "Loss:  0.6043428182601929\n",
      "Loss:  0.6534136533737183\n",
      "Loss:  0.5803978443145752\n",
      "Loss:  0.5124812722206116\n",
      "Loss:  0.5127522945404053\n",
      "Loss:  0.48466363549232483\n",
      "Loss:  0.6244211792945862\n",
      "Loss:  0.5820704102516174\n",
      "Loss:  0.641322672367096\n",
      "Loss:  0.5887343883514404\n",
      "Loss:  0.6451951265335083\n",
      "Loss:  0.5341012477874756\n",
      "Loss:  0.5574768781661987\n",
      "Loss:  0.40558212995529175\n",
      "Loss:  0.6386796832084656\n",
      "Loss:  0.5941553115844727\n",
      "Loss:  0.7421880960464478\n"
     ]
    }
   ],
   "source": [
    "# Попробуем на 20 эпохах\n",
    "for epoch in range(20):\n",
    "    for x_batch, y_batch in train_dataloader:\n",
    "        y_pred = net.forward(x_batch)\n",
    "        loss = loss_func(y_pred.squeeze(), y_batch.float())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print('Loss: ', loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7497831583023071"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(net, x_val, y_val):\n",
    "    y_predict = net.forward(x_val.float())\n",
    "    plt.plot(x_val.numpy(), y_val.numpy(), 'o', c = 'g', label = 'то что должно быть')\n",
    "    plt.plot(x_val.numpy(), y_predict.data.numpy(), 'o', c = 'r', label = 'predict of network')\n",
    "#    plt.legend(loc = 'upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "predict(our_net, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.0000, 47.0000,  0.0000,  0.0000,  9.0000],\n",
       "        [ 3.0000,  0.0000,  0.0000,  0.0000,  7.7500],\n",
       "        [ 2.0000, 34.0000,  1.0000,  0.0000, 26.0000],\n",
       "        ...,\n",
       "        [ 3.0000, 19.0000,  0.0000,  0.0000,  6.7500],\n",
       "        [ 3.0000,  0.0000,  1.0000,  0.0000, 15.5000],\n",
       "        [ 3.0000, 21.0000,  0.0000,  0.0000,  7.8542]])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.float().dtype\n",
    "X_train.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.float().dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adam (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    eps: 1e-08\n",
       "    lr: 0.0001\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Попробуем теперь на 1000 эпохах\n",
    "for epoch in range(1000):\n",
    "    optimizer.zero_grad()\n",
    "    X_train[]\n",
    "    y_val = our_net.forward(X_train.float())\n",
    "    y_val.squeeze_()\n",
    "    loss_val = loss_func(y_val, y_train.float())\n",
    "    loss_val.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "predict(our_net, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
